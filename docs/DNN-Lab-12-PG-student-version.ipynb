{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "39250c4d",
      "metadata": {
        "id": "39250c4d"
      },
      "source": [
        "# Policy Gradient\n",
        "\n",
        "In reinforcement learning, the goal is to find a policy $\\pi_\\theta(a|s)$ that maximizes the expected cumulative reward. Policy Gradient (PG) methods are a class of algorithms for finding such policies in which the policy is represented by a parameterized function, such as a neural network. The key idea behind PG methods is that the gradient of the expected reward can be rephrased as the expected gradient of rewards times log-probabilities (the log-d trick):\n",
        "\n",
        "$$ \\nabla_\\theta\\,\\operatorname*{\\mathbb{E}}_{s,\\ a \\sim \\pi_\\theta}\\, \\text{rewards} = \\operatorname*{\\mathbb{E}}_{s,\\ a \\sim \\pi_\\theta}\\, \\text{rewards} \\cdot \\nabla_\\theta \\log \\pi_\\theta(a|s) $$\n",
        "\n",
        "PG methods differ in:\n",
        "- how $ \\operatorname*{\\mathbb{E}}_{s,\\ a \\sim \\pi_\\theta}$ is defined and approximated (how trajectory samples are collected and batched in gradient descent steps),\n",
        "- what expression is used for *rewards* (with relative advantage estimates, value functions, etc.).\n",
        "\n",
        "In its simplest form, we estimate the gradient with a sample of recent transitions, which were sampled according to the policy, and the PG is equal to:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J = \\underset{s \\sim p^{\\pi}_{*}}{\\mathbb{E}} ~ \\underset{a \\sim \\pi}{\\mathbb{E}} ~ Q^{\\pi} (s, a) ~ \\nabla_{\\theta} \\log \\pi_{\\theta} (a | s)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $p^{\\pi}_{*}$ is the distribution of states obtained by rolling out the policy $\\pi_{\\theta}(a|s)$.\n",
        "- $Q^{\\pi} (s, a)$ denotes the discounted Q-value under policy.\n",
        "\n",
        "Other popular PG variants are the Actor-Critic and PPO.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "1. Run the **PolicyGradient** agent and plot mean+std of rewards (as it progresses through training).\n",
        "2. Implement the *compute_pseudo_loss* method for the **BaselinedPolicyGradient** class.\n",
        "3. Implement the *compute_pseudo_loss* method for the **ActorCritic** class.\n",
        "\n",
        "Each training takes ~3 minutes on Colab CPU (GPUs won't help)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "a318632a",
      "metadata": {
        "id": "a318632a"
      },
      "outputs": [],
      "source": [
        "# %pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "8ceeec92",
      "metadata": {
        "id": "8ceeec92"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import SupportsFloat\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch import Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e61edada",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e61edada",
        "outputId": "19ecd7fd-efc0-43f1-b5dd-1c85a20ebd17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False cpu\n"
          ]
        }
      ],
      "source": [
        "# Check for CUDA / MPS (Apple) / XPU (Intel) / ... accelerator.\n",
        "device = torch.accelerator.current_accelerator(True) or torch.device(\"cpu\")\n",
        "use_accel = device != torch.device(\"cpu\")\n",
        "print(use_accel, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7db94c6",
      "metadata": {
        "id": "f7db94c6"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "6f47d27a",
      "metadata": {
        "id": "6f47d27a"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    # Do not change!\n",
        "\n",
        "    gym_id: str = \"CartPole-v1\"\n",
        "    buffer_size: int = 32  # Number of transitions to store in the transition buffer.\n",
        "    hidden_dim: int = 32  # Size of hidden layer in policy and value networks.\n",
        "    policy_learning_rate: float = 5e-3\n",
        "    value_learning_rate: float = 1e-2\n",
        "    discount: float = 0.99  # Gamma discount factor in definition of Q and V values.\n",
        "    n_train_steps: int = 10_000\n",
        "    n_trainings: int = 10\n",
        "    n_eval_samples: int = 20  # Number of episodes rolled-out in each evaluation.\n",
        "    steps_between_evals: int = 2000  # How often to evaluate during training.\n",
        "    device: torch.device = device\n",
        "\n",
        "    def __post_init__(self):\n",
        "        env = gym.make(self.gym_id)\n",
        "\n",
        "        # Observation, consisting of: position, velocity, angle, angular velocity.\n",
        "        assert isinstance(env.observation_space, gym.spaces.Box)\n",
        "        self.obs_size = env.observation_space.shape[0]\n",
        "\n",
        "        # Two actions: left and right.\n",
        "        assert isinstance(env.action_space, gym.spaces.Discrete)\n",
        "        self.action_size = int(env.action_space.n)\n",
        "\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1265f784",
      "metadata": {
        "id": "1265f784"
      },
      "source": [
        "## Plotter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "4afd2b81",
      "metadata": {
        "id": "4afd2b81"
      },
      "outputs": [],
      "source": [
        "class Plotter:\n",
        "    \"\"\"Collects and plots rewards curves for each experiment.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config) -> None:\n",
        "        self.config = config\n",
        "        # Maps experiment name to list of curves (from different training seeds).\n",
        "        # Each curve is a list of mean total rewards in evaluations done during training.\n",
        "        self.data = dict[str, list[list[float]]]()\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.data = dict[str, list[list[float]]]()\n",
        "\n",
        "    def add_rewards_curve(self, label: str, rewards_curve: list[float]) -> None:\n",
        "        \"\"\"Plot any number of reward curves, .\"\"\"\n",
        "        if label not in self.data:\n",
        "            self.data[label] = []\n",
        "        self.data[label].append(rewards_curve)\n",
        "\n",
        "    def show(self) -> None:\n",
        "        for label, rewards_list in self.data.items():\n",
        "            average_rewards = np.mean(rewards_list, axis=0)\n",
        "\n",
        "            x = np.linspace(0, 100, len(average_rewards))\n",
        "            plt.plot(x, average_rewards, label=label)\n",
        "\n",
        "        plt.ylim(bottom=0, top=400)\n",
        "        plt.legend(loc=\"upper left\")\n",
        "        plt.xlabel(\"Training Progress [%]\")\n",
        "        plt.ylabel(\"Reward (mean across trainings)\")\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "plotter = Plotter(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "263c8775",
      "metadata": {
        "id": "263c8775"
      },
      "source": [
        "## Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a2a605",
      "metadata": {
        "id": "d6a2a605"
      },
      "source": [
        "We define a policy network (**ActorNetwork**) and a value network (**CriticNetwork**), with helper methods to work directly with numpy.ndarray observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8d97a2b8",
      "metadata": {
        "id": "8d97a2b8"
      },
      "outputs": [],
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    def __init__(self, config: Config) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(config.obs_size, config.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.hidden_dim, config.action_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, obs: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Input: observation, shape (batch_size, obs_size).\n",
        "        Output: logits for action probabilities, shape (batch_size, action_size).\n",
        "        \"\"\"\n",
        "        return self.layers(obs)  # No softmax here.\n",
        "\n",
        "    def sample_action(self, obs: np.ndarray) -> tuple[Tensor, Tensor]:\n",
        "        \"\"\"\n",
        "        Sample an action for a given observation of shape (batch_size, obs_size).\n",
        "\n",
        "        Returns:\n",
        "        - action: sampled action, shape (batch_size,), dtype int.\n",
        "        - logprob: log prob. of that action, shape (batch_size,), differentiable wrt actor params.\n",
        "        \"\"\"\n",
        "        batch_size, _obs_size = obs.shape\n",
        "        device = next(self.parameters()).device\n",
        "        logits = self(Tensor(obs, device=device))\n",
        "        dist = torch.distributions.Categorical(logits=logits)\n",
        "        action = dist.sample((batch_size,))\n",
        "        return action, dist.log_prob(action)\n",
        "\n",
        "\n",
        "class CriticNetwork(nn.Module):\n",
        "    def __init__(self, config: Config) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(config.obs_size, config.hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config.hidden_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, obs: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Input: observation, shape (batch_size, obs_size).\n",
        "        Output: V value estimate, shape (batch_size,).\n",
        "        \"\"\"\n",
        "        return torch.squeeze(self.layers(obs), -1)\n",
        "\n",
        "    def get_value(self, obs: np.ndarray) -> Tensor:\n",
        "        \"\"\"Given obs of shape (batch_size, obs_size), return value estimate, shape (batch_size,).\"\"\"\n",
        "        device = next(self.parameters()).device\n",
        "        return self(Tensor(obs, device=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "183095c8",
      "metadata": {
        "id": "183095c8"
      },
      "source": [
        "## Buffer\n",
        "We maintain a buffer (a.k.a. batch) of recent transitions, to make updates averaged over multiple transitions.\n",
        "The buffer is small (buffer_size=32 vs episode length ~200), not like the experience replay buffer from DQN.\n",
        "We add to it until it is full, use it to make an update, and then clear it; repeat.\n",
        "\n",
        "The buffer defines a method for calculating Q-values: as sums of rewards (discounted).\n",
        "Since the buffer is small, we will never have a full episode in the buffer, so we need an estimate of rewards post final state in the buffer.\n",
        "We get such an estimate from the value network (critic).\n",
        "This will be the only use of the value network, for the first algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "39231fdd",
      "metadata": {
        "id": "39231fdd"
      },
      "outputs": [],
      "source": [
        "class Buffer:\n",
        "    def __init__(self, buffer_size: int, device: torch.device) -> None:\n",
        "        self.buffer_size = buffer_size\n",
        "        self.device = device\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        buffer_size, device = self.buffer_size, self.device\n",
        "        self.logprobs = torch.zeros(buffer_size, device=device)\n",
        "        self.values = torch.zeros(buffer_size, device=device)\n",
        "        self.rewards = torch.zeros(buffer_size, device=device)\n",
        "        self.terminals = torch.zeros(buffer_size, dtype=torch.bool, device=device)\n",
        "        self.idx = 0\n",
        "\n",
        "    def add(\n",
        "        self,\n",
        "        logprob: Tensor,\n",
        "        value: Tensor,\n",
        "        reward: SupportsFloat,\n",
        "        terminal: SupportsFloat,\n",
        "    ) -> None:\n",
        "        \"\"\"Called after each train episode step (with differentiable logprob and value scalars).\"\"\"\n",
        "        self.logprobs[self.idx] = logprob\n",
        "        self.values[self.idx] = value\n",
        "        self.rewards[self.idx] = float(reward)\n",
        "        self.terminals[self.idx] = float(terminal)\n",
        "        self.idx += 1\n",
        "\n",
        "    def get_qvalues(self, final_value: Tensor, discount: float = 1.0) -> Tensor:\n",
        "        \"\"\"\n",
        "        Return Q-values for the buffer (discounted sum of rewards), shape (buffer_size,).\n",
        "\n",
        "        Args:\n",
        "        - final_value: V value estimate for the final state, scalar.\n",
        "        - discount: gamma in the definition of Q-values.\n",
        "        \"\"\"\n",
        "        q_values = torch.zeros_like(self.rewards).to(self.rewards.device)\n",
        "\n",
        "        next_value = final_value\n",
        "        for timestep in reversed(range(self.buffer_size)):\n",
        "            if self.terminals[timestep]:\n",
        "                next_value = 0.0\n",
        "\n",
        "            q_values[timestep] = self.rewards[timestep] + discount * next_value\n",
        "            next_value = q_values[timestep]\n",
        "\n",
        "        return q_values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ec42f68",
      "metadata": {
        "id": "4ec42f68"
      },
      "source": [
        "## PolicyGradient trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "585f0e73",
      "metadata": {
        "id": "585f0e73"
      },
      "outputs": [],
      "source": [
        "class PolicyGradient:\n",
        "    def __init__(self, config: Config) -> None:\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.buffer = Buffer(config.buffer_size, device=config.device)\n",
        "        self.init_networks()\n",
        "\n",
        "    def init_networks(self) -> None:\n",
        "        self.actor = ActorNetwork(self.config).to(self.config.device)\n",
        "        self.critic = CriticNetwork(self.config).to(self.config.device)\n",
        "        self.actor_optimizer = optim.Adam(\n",
        "            self.actor.parameters(),\n",
        "            lr=self.config.policy_learning_rate,\n",
        "        )\n",
        "        self.critic_optimizer = optim.Adam(\n",
        "            self.critic.parameters(),\n",
        "            lr=self.config.value_learning_rate,\n",
        "        )\n",
        "        self.buffer.reset()\n",
        "\n",
        "    def evaluate(self) -> float:\n",
        "        \"\"\"Rollout `samples` episodes. Return the average total reward.\"\"\"\n",
        "        self.actor.eval()\n",
        "        self.critic.eval()\n",
        "        with torch.no_grad():\n",
        "            env_test = gym.make(self.config.gym_id)\n",
        "            total_reward = 0.0\n",
        "            for _ in range(self.config.n_eval_samples):\n",
        "                obs, _info = env_test.reset()\n",
        "                episode_reward = 0.0\n",
        "                while True:\n",
        "                    action, _logprobs = self.actor.sample_action(obs[np.newaxis, :])\n",
        "                    next_obs, reward, terminal, truncated, _info = env_test.step(\n",
        "                        action.item()\n",
        "                    )\n",
        "                    done = terminal or truncated\n",
        "                    episode_reward += float(reward)\n",
        "                    obs = next_obs\n",
        "                    if done:\n",
        "                        break\n",
        "                total_reward += episode_reward\n",
        "        return total_reward / self.config.n_eval_samples\n",
        "\n",
        "    def train(self, seed: int) -> list[float]:\n",
        "        \"\"\"Trains the agent and returns (timesteps // steps_between_evals) evaluation rewards.\"\"\"\n",
        "        env = gym.make(self.config.gym_id)\n",
        "        obs, _ = env.reset(seed=seed)\n",
        "\n",
        "        start_time = time.time()\n",
        "        eval_rewards = list[float]()\n",
        "\n",
        "        for step in range(config.n_train_steps):\n",
        "            self.actor.train()\n",
        "            self.critic.train()\n",
        "\n",
        "            value = self.critic.get_value(obs[np.newaxis, :])\n",
        "            action, logprob = self.actor.sample_action(obs[np.newaxis, :])\n",
        "            next_obs, reward, terminal, truncated, _ = env.step(action.item())\n",
        "            done = terminal or truncated\n",
        "            self.buffer.add(logprob, value, reward, done)\n",
        "            obs = next_obs\n",
        "\n",
        "            if self.buffer.idx == self.config.buffer_size:\n",
        "                self.update(obs)\n",
        "\n",
        "            if done:\n",
        "                obs, _info = env.reset(seed=seed)\n",
        "\n",
        "            # Evaluate every steps_between_evals.\n",
        "            if (step + 1) % self.config.steps_between_evals == 0:\n",
        "                eval_reward = self.evaluate()\n",
        "                eval_rewards.append(eval_reward)\n",
        "                samples_per_sec = (step + 1) / (time.time() - start_time)\n",
        "                if (step + 1) % (10 * self.config.steps_between_evals) == 0:\n",
        "                    print(f\"{step+1=}, {eval_reward=:.1f}, {samples_per_sec=:.0f}\")\n",
        "\n",
        "        return eval_rewards\n",
        "\n",
        "    def update(self, final_obs: np.ndarray) -> None:\n",
        "        # Final-value and hence target q-values depend on critic parameters,\n",
        "        # but we do not want to backprop through it\n",
        "        # (neither in the critic nor in the actor update)!\n",
        "        with torch.no_grad():\n",
        "            final_value = self.critic.get_value(final_obs[np.newaxis, :])\n",
        "            q_values = self.buffer.get_qvalues(final_value, self.config.discount)\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        value_loss = (self.buffer.values - q_values).pow(2).mean()\n",
        "        value_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        pseudo_loss = self.compute_pseudo_loss(q_values)\n",
        "        pseudo_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        self.buffer.reset()\n",
        "\n",
        "    def compute_pseudo_loss(self, q_values: Tensor) -> Tensor:\n",
        "        \"\"\"Compute pseudo-loss for the policy (actor) network, using the collected buffer.\"\"\"\n",
        "        return -(q_values * self.buffer.logprobs).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "w2Y3YZFFQVRz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "w2Y3YZFFQVRz",
        "outputId": "9696c9f6-47eb-43df-fff9-f3f1a86f18b0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "for i in range(config.n_trainings):\n",
        "    plotter.add_rewards_curve(\"vanilla\", PolicyGradient(config).train(i))\n",
        "plotter.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "270d8fc8",
      "metadata": {
        "id": "270d8fc8"
      },
      "source": [
        "## PG with baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f33e74",
      "metadata": {
        "id": "c0f33e74"
      },
      "source": [
        "The vanilla PG estimate has big variance  it will vary a lot depending on the states and actions sampled.\n",
        "This variance is known to impact the sample efficiency and final performance of the agent.\n",
        "Although there are many strategies fo PG variance reduction, there is one trick that became indispensable in modern implementations of PG  the baseline. But before we get to this, lets have one more look at the vanilla PG estimate:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J = \\underset{s \\sim p^{\\pi}_{*}}{\\mathbb{E}} ~ \\underset{a \\sim \\pi}{\\mathbb{E}} ~ Q^{\\pi} (s, a) ~ \\nabla_{\\theta} \\log \\pi_{\\theta} (a | s)\n",
        "$$\n",
        "\n",
        "As discussed in the lecture, the idea of PG is quite simple  increase the probability of good actions and decrease the probability of bad ones.\n",
        "'Good' and 'bad' here refer to Q-values associated with given actions. But what if all Q-values are positive? The gradient update will try to increase the logits of all sampled actions, with the increase being proportional to the Q-value (as stems from the equation above). As such, given positive Q-values, the logits of bad actions are also increased  just by a smaller amount that logits of actions with bigger Q-values.\n",
        "\n",
        "Baseline variance reduction tackles exactly that. The idea is to subtract a \"baseline\" from the Q-values, which does not affect the optimal policy, but reduces the variance of the gradients. It is proven that as long as the baseline does not depend on the action, its value will not bias the PG (a bad choice may increase the variance, though :)). So, what is a good baseline?\n",
        "\n",
        "The simplest version subtracts batch average of Q-values from each Q-value:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J = \\sum_{i=1}^{B} \\frac{1}{B} \\Bigl( Q^{\\pi} (s_i, a_i) - \\frac{\\sum Q^{\\pi} (s_i, a_i)}{B} \\Bigr) \\nabla_{\\theta} \\log \\pi_{\\theta} (a_i | s_i)\n",
        "$$\n",
        "\n",
        "Where B is the batch size and $\\frac{\\sum Q^{\\pi} (s_i, a_i)}{B}$ is the average Q-value in the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "4a145a52",
      "metadata": {
        "id": "4a145a52"
      },
      "outputs": [],
      "source": [
        "class BaselinedPolicyGradient(PolicyGradient):\n",
        "    def compute_pseudo_loss(self, q_values: Tensor) -> Tensor:\n",
        "        ## TODO {\n",
        "        q_values = q_values - q_values.mean()\n",
        "        \"\"\"Compute pseudo-loss for the policy (actor) network, using the collected buffer.\"\"\"\n",
        "        return -(q_values * self.buffer.logprobs).mean()\n",
        "        ## }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bf6d9f9",
      "metadata": {
        "id": "0bf6d9f9"
      },
      "outputs": [],
      "source": [
        "for i in range(config.n_trainings):\n",
        "    plotter.add_rewards_curve(\"mean baseline\", BaselinedPolicyGradient(config).train(i))\n",
        "plotter.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f6bd3e0",
      "metadata": {
        "id": "8f6bd3e0"
      },
      "source": [
        "## Actor-Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "729f5f80",
      "metadata": {
        "id": "729f5f80"
      },
      "source": [
        "The most popular approach is to use the state V value, estimated by a value network (*CriticNetwork*), as the baseline. Then, the gradient is well centered  probabilities of actions with Q-values smaller than state value will be decreased, while probabilities of actions with Q-values bigger than state value will be increased. Traditionally, PG variants that use a value network for baseline variance reduction are called Actor-Critic. The Actor-Critic update is calculated as:\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta} J = \\sum_{i=1}^{B} \\frac{1}{B} \\bigl( Q^{\\pi} (s_i, a_i) - V_{\\phi}(s) \\bigr) \\nabla_{\\theta} \\log \\pi_{\\theta} (a_i | s_i)\n",
        "$$\n",
        "\n",
        "Where $\\bigl( Q^{\\pi} (s_i, a_i) - V_{\\phi}(s) \\bigr)$ is referred to as the **advantage**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "261d7dfc",
      "metadata": {
        "id": "261d7dfc"
      },
      "outputs": [],
      "source": [
        "class ActorCritic(PolicyGradient):\n",
        "    def compute_pseudo_loss(self, q_values: Tensor) -> Tensor:\n",
        "        ## TODO {\n",
        "        q_values = q_values - self.buffer.rewards\n",
        "        \"\"\"Compute pseudo-loss for the policy (actor) network, using the collected buffer.\"\"\"\n",
        "        return -(q_values * self.buffer.logprobs).mean()\n",
        "        ## }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4df0c7c4",
      "metadata": {
        "id": "4df0c7c4"
      },
      "outputs": [],
      "source": [
        "for i in range(config.n_trainings):\n",
        "    plotter.add_rewards_curve(\"V baseline\", ActorCritic(config).train(i))\n",
        "plotter.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "colab",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}